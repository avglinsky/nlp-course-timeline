{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "64c2ac4d",
   "metadata": {},
   "source": [
    "# **Важно!** \n",
    "\n",
    "Домашнее задание состоит из нескольких задач, которые вам нужно решить.\n",
    "*   Баллы выставляются по принципу выполнено/невыполнено.\n",
    "*   За каждую выполненую задачу вы получаете баллы (количество баллов за задание указано в скобках).\n",
    "\n",
    "**Инструкция выполнения:** Выполните задания в этом же ноутбуке (места под решения **КАЖДОЙ** задачи обозначены как **#НАЧАЛО ВАШЕГО РЕШЕНИЯ** и **#КОНЕЦ ВАШЕГО РЕШЕНИЯ**)\n",
    "\n",
    "**Как отправить задание на проверку:** Вам необходимо сохранить ваше решение в данном блокноте и отправить итоговый **файл .IPYNB** на учебной платформе в **стандартную форму сдачи домашнего задания.**\n",
    "\n",
    "**Срок проверки преподавателем:** домашнее задание проверяется **в течение 3 дней после дедлайна сдачи** с предоставлением обратной связи\n",
    "\n",
    "# **Прежде чем проверять задания:**\n",
    "\n",
    "1. Перезапустите **ядро (restart the kernel)**: в меню, выбрать **Ядро (Kernel)**\n",
    "→ **Перезапустить (Restart)**\n",
    "2. Затем **Выполнить** **все ячейки (run all cells)**: в меню, выбрать **Ячейка (Cell)**\n",
    "→ **Запустить все (Run All)**.\n",
    "\n",
    "После ячеек с заданием следуют ячейки с проверкой **с помощью assert.**\n",
    "\n",
    "Если в коде есть ошибки, assert выведет уведомление об ошибке.\n",
    "\n",
    "Если в коде нет ошибок, assert отработает без вывода дополнительной информации."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1f1f5d5",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1362be6-7798-4b54-b699-d6cf48225a09",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "7dd730f5bf91fbba5e436edd9af9a14b",
     "grade": false,
     "grade_id": "cell-2c49a34fff1c4c7c",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "class AdamOptimizer:\n",
    "    \"\"\"\n",
    "    Задание 1 (1 балл): \n",
    "\n",
    "    Напишите метод `__init__` для класса `AdamOptimizer`.\n",
    "    \n",
    "    **Описание задания:**\n",
    "    \n",
    "    Вы должны создать конструктор класса `AdamOptimizer`, который инициализирует параметры оптимизатора Adam.\n",
    "    \n",
    "    **Параметры конструктора:**\n",
    "    1. `learning_rate` (по умолчанию 0.001): Скорость обучения (learning rate) оптимизатора Adam.\n",
    "    2. `beta1` (по умолчанию 0.9): Коэффициент для экспоненциального скользящего среднего первого порядка.\n",
    "    3. `beta2` (по умолчанию 0.999): Коэффициент для экспоненциального скользящего среднего второго порядка.\n",
    "    4. `epsilon` (по умолчанию 1e-8): Малое число для предотвращения деления на ноль.\n",
    "    \n",
    "    **Поля экземпляра класса:**\n",
    "    1. `learning_rate`: Скорость обучения.\n",
    "    2. `beta1`: Значение `beta1`.\n",
    "    3. `beta2`: Значение `beta2`.\n",
    "    4. `epsilon`: Значение `epsilon`.\n",
    "    5. `momentums`: Инициализируется как `None`. Здесь будет храниться экспоненциальное скользящее среднее первого порядка.\n",
    "    6. `running_squared_gradients`: Инициализируется как `None`. Здесь будет храниться экспоненциальное скользящее среднее второго порядка.\n",
    "    7. `t`: Инициализируется как 0. Это счетчик шагов оптимизации.\n",
    "    \n",
    "    **Пример использования:**\n",
    "    \n",
    "    # Создаем объект AdamOptimizer с заданными параметрами\n",
    "    optimizer = AdamOptimizer(learning_rate=0.001, beta1=0.9, beta2=0.999, epsilon=1e-8)\n",
    "    \"\"\"\n",
    "    # НАЧАЛО ВАШЕГО РЕШЕНИЯ\n",
    "    raise NotImplementedError() # удалить эту строку в процессе решения\n",
    "    # КОНЕЦ ВАШЕГО РЕШЕНИЯ\n",
    "\n",
    "    \"\"\"\n",
    "    Задание 2 (2 балла):\n",
    "\n",
    "    **Задача:** Напишите метод `initialize` для класса `AdamOptimizer`.\n",
    "    \n",
    "    **Описание задания:**\n",
    "    \n",
    "    Вам необходимо реализовать метод `initialize`, который будет выполнять инициализацию моментов первого и второго порядка для каждого параметра оптимизатора Adam.\n",
    "    \n",
    "    **Параметры метода:**\n",
    "    1. `parameters` - Список, содержащий параметры, которые нужно инициализировать.\n",
    "    \n",
    "    **Действия метода:**\n",
    "    1. Проверьте, что переданный список `parameters` не пустой. Если список пустой, выбросьте исключение с текстом \"Необходимо передать параметры\".\n",
    "    \n",
    "    2. Для каждого параметра в списке `parameters` создайте массивы нулей с такой же формой и типом данных, как у соответствующего параметра. Эти массивы будут использоваться для хранения моментов первого и второго порядка.\n",
    "    \n",
    "    3. Присвойте полученные массивы нулей атрибутам `momentums` и `running_squared_gradients` класса `AdamOptimizer`. \n",
    "    \n",
    "    **Пример использования:**\n",
    "    # Создаем объект AdamOptimizer\n",
    "    optimizer = AdamOptimizer(learning_rate=0.001)\n",
    "    \n",
    "    # Инициализируем моменты для списка параметров\n",
    "    parameters = [np.array([1.0, 2.0]), np.array([3.0, 4.0])]\n",
    "    optimizer.initialize(parameters)\n",
    "    \"\"\"\n",
    "    # НАЧАЛО ВАШЕГО РЕШЕНИЯ\n",
    "    raise NotImplementedError() # удалить эту строку в процессе решения\n",
    "    # КОНЕЦ ВАШЕГО РЕШЕНИЯ\n",
    "\n",
    "    \"\"\"\n",
    "    Задание 3 (4 балла):\n",
    "\n",
    "    **Задача:** Напишите метод `update` для класса `AdamOptimizer`.\n",
    "    \n",
    "    **Описание задания:**\n",
    "    \n",
    "    Вам необходимо реализовать метод `update`, который будет выполнять обновление параметров с использованием оптимизатора Adam.\n",
    "    \n",
    "    **Параметры метода:**\n",
    "    1. `parameters` - Список, содержащий параметры модели, которые нужно обновить.\n",
    "    2. `gradients` - Список, содержащий градиенты параметров, вычисленные на текущем шаге оптимизации.\n",
    "    \n",
    "    **Действия метода:**\n",
    "    1. Проверьте атрибут `momentums` оптимизатора на равенство `None`. \n",
    "    Если `momentums` равен `None`, вызовите self.initialize(parameters)\n",
    "    \n",
    "    2. Проверьте, что список `gradients` не пустой. \n",
    "    Если `gradients` пустой, выбросьте исключение с текстом \"Необходимо передать непустой градиент\".\n",
    "    \n",
    "    3. Увеличьте значение атрибута `t` на 1.\n",
    "    \n",
    "    4. Вычислите скорость обучения `learning_rate_t` на текущем шаге, используя формулу Adam.\n",
    "    learning_rate_t = self.learning_rate * np.sqrt(1 - self.beta2 ** self.t) / (1 - self.beta1 ** self.t)\n",
    "    \n",
    "    5. Для каждого параметра в списке `parameters` выполните следующие шаги:\n",
    "       - Обновите момент первого порядка (экспоненциальное скользящее среднее градиента).\n",
    "       self.momentums[i] = self.beta1 * self.momentums[i] + (1 - self.beta1) * gradients[i]\n",
    "       \n",
    "       - Обновите момент второго порядка (экспоненциальное скользящее среднее квадрата градиента).\n",
    "       self.running_squared_gradients[i] = self.beta2 * self.running_squared_gradients[i] + \\\n",
    "                                                (1 - self.beta2) * gradients[i] ** 2\n",
    "                                                \n",
    "       - Вычислите скорость обучения `learning_rate_i` для данного параметра, используя формулу Adam.\n",
    "       learning_rate_i = learning_rate_t / (np.sqrt(self.running_squared_gradients[i]) + self.epsilon)\n",
    "       \n",
    "       - Обновите значение параметра, учитывая моменты и скорость обучения.\n",
    "       parameters[i] -= learning_rate_i * self.momentums[i]\n",
    "    \n",
    "    **Пример использования:**\n",
    "    # Создаем объект AdamOptimizer с заданными параметрами\n",
    "    optimizer = AdamOptimizer(learning_rate=0.001, beta1=0.9, beta2=0.999, epsilon=1e-8)\n",
    "    \n",
    "    # Инициализируем моменты для списка параметров\n",
    "    parameters = [np.array([1.0, 2.0]), np.array([3.0, 4.0])]\n",
    "    optimizer.initialize(parameters)\n",
    "    \n",
    "    # Вычисляем градиенты\n",
    "    gradients = [np.array([0.1, 0.2]), np.array([-0.3, -0.4])]\n",
    "    \n",
    "    # Обновляем параметры с использованием метода update\n",
    "    optimizer.update(parameters, gradients)\n",
    "    \"\"\"\n",
    "    \n",
    "    # НАЧАЛО ВАШЕГО РЕШЕНИЯ\n",
    "    raise NotImplementedError() # удалить эту строку в процессе решения\n",
    "    # КОНЕЦ ВАШЕГО РЕШЕНИЯ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3835180d-12c9-4403-a4c1-16ff95d50797",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "0181c4c82c11f23aec2b31f98b87369c",
     "grade": true,
     "grade_id": "cell-71db6cb4373cb3e2",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "optimizer = AdamOptimizer()\n",
    "assert optimizer.learning_rate == 0.001\n",
    "assert optimizer.beta1 == 0.9\n",
    "assert optimizer.beta2 == 0.999\n",
    "assert optimizer.epsilon == 1e-8\n",
    "assert optimizer.momentums is None\n",
    "assert optimizer.running_squared_gradients is None\n",
    "assert optimizer.t == 0\n",
    "\n",
    "optimizer = AdamOptimizer(learning_rate=0.01, beta1=0.95, beta2=0.99, epsilon=1e-6)\n",
    "assert optimizer.learning_rate == 0.01\n",
    "assert optimizer.beta1 == 0.95\n",
    "assert optimizer.beta2 == 0.99\n",
    "assert optimizer.epsilon == 1e-6\n",
    "assert optimizer.momentums is None\n",
    "assert optimizer.running_squared_gradients is None\n",
    "assert optimizer.t == 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "897f99e6-e408-4331-8b69-852a3f3a1379",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "4e524eacbd6ac9dfbc7ca7d91a826998",
     "grade": true,
     "grade_id": "cell-99e8a66fa82093da",
     "locked": true,
     "points": 2,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# обычное тестирование\n",
    "optimizer = AdamOptimizer()\n",
    "parameters = [np.array([1.0, 2.0]), np.array([3.0, 4.0])]\n",
    "optimizer.initialize(parameters)\n",
    "\n",
    "for i in range(len(parameters)):\n",
    "    assert np.all(optimizer.momentums[i] == np.zeros_like(parameters[i]))\n",
    "    assert np.all(optimizer.running_squared_gradients[i] == np.zeros_like(parameters[i]))\n",
    "\n",
    "# инициализация параметров не с помощью numpy\n",
    "optimizer = AdamOptimizer()\n",
    "parameters = [np.array([1.0, 2.0]), np.array([3.0, 4.0])]\n",
    "optimizer.initialize(parameters)\n",
    "\n",
    "for i in range(len(parameters)):\n",
    "    assert np.all(optimizer.momentums[i] == np.zeros_like(parameters[i]))\n",
    "    assert np.all(optimizer.running_squared_gradients[i] == np.zeros_like(parameters[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f31072f2-b2cf-4325-aad6-f4567dde3ee9",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "17affa39898a93250bc72ba758c75670",
     "grade": true,
     "grade_id": "cell-7110baa3a3ec00fd",
     "locked": true,
     "points": 4,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "optimizer = AdamOptimizer(learning_rate=0.001)\n",
    "parameters = [np.array([1.0, 2.0]), np.array([3.0, 4.0])]\n",
    "gradients = [np.array([0.1, 0.2]), np.array([-0.3, -0.4])]\n",
    "\n",
    "# Тестовый случай 1: Проверка корректности обновления параметров\n",
    "optimizer.update(parameters, gradients)\n",
    "\n",
    "parameters\n",
    "assert np.allclose(parameters[0], np.array([0.999, 1.999]), rtol=1e-5)\n",
    "assert np.allclose(parameters[1], np.array([3.001, 4.001]), rtol=1e-5)\n",
    "\n",
    "# Тестовый случай 2: Проверка корректного инкремента t\n",
    "assert optimizer.t == 1\n",
    "\n",
    "# Тестовый случай 3: Проверка правильности вычисления скорости обучения\n",
    "assert np.isclose(optimizer.learning_rate, 0.001)\n",
    "\n",
    "# Тестовый случай 4: Проверка возникновения исключения при пустом градиенте\n",
    "try:\n",
    "    optimizer.update(parameters, [])\n",
    "except Exception as e:\n",
    "    assert str(e) == \"Необходимо передать непустой градиент\"\n",
    "\n",
    "# Тестовый случай 5: Проверка возникновения исключения при неинициализированном оптимизаторе\n",
    "optimizer = AdamOptimizer(learning_rate=0.001)\n",
    "try:\n",
    "    optimizer.update([], [np.array([0.1, 0.2])])\n",
    "except Exception as e:\n",
    "    assert str(e) == \"Необходимо передать параметры\"\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
