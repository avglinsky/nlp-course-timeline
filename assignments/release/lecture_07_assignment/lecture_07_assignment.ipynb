{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Важно!** \n",
    "\n",
    "Домашнее задание состоит из нескольких задач, которые вам нужно решить.\n",
    "*   Баллы выставляются по принципу выполнено/невыполнено.\n",
    "*   За каждую выполненую задачу вы получаете баллы (количество баллов за задание указано в скобках).\n",
    "\n",
    "**Инструкция выполнения:** Выполните задания в этом же ноутбуке (места под решения **КАЖДОЙ** задачи обозначены как **#НАЧАЛО ВАШЕГО РЕШЕНИЯ** и **#КОНЕЦ ВАШЕГО РЕШЕНИЯ**)\n",
    "\n",
    "**Как отправить задание на проверку:** Вам необходимо сохранить ваше решение в данном блокноте и отправить итоговый **файл .IPYNB** на учебной платформе в **стандартную форму сдачи домашнего задания.**\n",
    "\n",
    "**Срок проверки преподавателем:** домашнее задание проверяется **в течение 3 дней после дедлайна сдачи** с предоставлением обратной связи\n",
    "\n",
    "# **Прежде чем проверять задания:**\n",
    "\n",
    "1. Перезапустите **ядро (restart the kernel)**: в меню, выбрать **Ядро (Kernel)**\n",
    "→ **Перезапустить (Restart)**\n",
    "2. Затем **Выполнить** **все ячейки (run all cells)**: в меню, выбрать **Ячейка (Cell)**\n",
    "→ **Запустить все (Run All)**.\n",
    "\n",
    "После ячеек с заданием следуют ячейки с проверкой **с помощью assert.**\n",
    "\n",
    "Если в коде есть ошибки, assert выведет уведомление об ошибке.\n",
    "\n",
    "Если в коде нет ошибок, assert отработает без вывода дополнительной информации."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import re\n",
    "from random import randrange, shuffle, random, randint\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "6d86a1487b0c6217219e8a8543a371a1",
     "grade": true,
     "grade_id": "cell-71dc2f298aaab2b9",
     "locked": false,
     "points": 0,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Задание 1 (1 балл): Генерация пакетов данных для обучения модели BERT\n",
    "\n",
    "**Цель задания:** Написать код, который генерирует пакеты данных для обучения модели BERT с использованием Python и библиотеки NumPy.\n",
    "\n",
    "**Задачи:**\n",
    "1. Сформировать пустой список `batch` для хранения пакетов данных.\n",
    "2. Инициализировать счетчики `positive` и `negative` для отслеживания положительных и отрицательных пар предложений.\n",
    "3. В цикле, пока количество положительных и отрицательных пар в `batch` не достигнет заданного размера (`batch_size/2`), выполнить следующие шаги:\n",
    "    - Выбрать случайные индексы двух предложений из списка `sentences`.\n",
    "    - Получить сами предложения по выбранным индексам и сохранить их в переменные `tokens_a` и `tokens_b`.\n",
    "    - Собрать входные данные для модели BERT, добавив специальные токены `[CLS]` и `[SEP]`. Используйте переменные `input_ids` и `segment_ids` \n",
    "    для этой цели.\n",
    "    - Реализовать маскирование токенов (MASK LM):\n",
    "        - Определить количество токенов, которые будут маскированы (подставлены вместо них `[MASK]`).\n",
    "        - Создать список кандидатов для маскирования (`cand_maked_pos`) и перемешать его.\n",
    "        - Заполнить списки `masked_tokens` и `masked_pos` маскированными токенами и их позициями.\n",
    "        - С вероятностью 80%, замаскировать токен, а с вероятностью 10%, заменить его случайным словом из словаря.\n",
    "    - Дополнить последовательности нулями (`0`) до максимальной длины (`maxlen`).\n",
    "    - Дополнить нулями маскированные токены и их позиции, чтобы их количество соответствовало максимально возможному (`max_pred`).\n",
    "    - Определить, являются ли два предложения последовательными (`IsNext`) или нет (`NotNext`) на основе индексов предложений и счетчиков `positive`\n",
    "    и `negative`.\n",
    "    - Добавить полученные данные в список `batch` как положительную пару (`IsNext`) или отрицательную пару (`NotNext`).\n",
    "    - Увеличить соответствующий счетчик (`positive` или `negative`) на `1`.\n",
    "4. Вернуть сформированный список `batch` как результат выполнения функции.\n",
    "\n",
    "**Примечания:**\n",
    "- Задания выполняются в рамках задачи Next Sentence Prediction (NSP) для обучения модели BERT.\n",
    "- Вы можете использовать функции и библиотеки Python, такие как `randrange`, `shuffle`, `random`, `randint`, `extend` и другие, для реализации\n",
    "задачи.\n",
    "- Весь необходимый функционал для работы с данными и списками будет предоставлен вам. Ваша задача - реализовать собственно формирование\n",
    "пакетов данных на основе предоставленного кода и комментариев.\n",
    "\n",
    "В случае затруднений можно обратиться к:\n",
    "https://github.com/graykode/nlp-tutorial/blob/master/5-2.BERT/BERT.py\n",
    "\"\"\"\n",
    "\n",
    "def make_batch():\n",
    "    # НАЧАЛО ВАШЕГО РЕШЕНИЯ\n",
    "    raise NotImplementedError() # удалить эту строку в процессе решения\n",
    "    # КОНЕЦ ВАШЕГО РЕШЕНИЯ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_attn_pad_mask(seq_q, seq_k):\n",
    "    \"\"\"\n",
    "    Функция для создания маски внимания, которая скрывает внимание на токенах-заполнителях (PAD).\n",
    "    \n",
    "    Args:\n",
    "    - seq_q: Тензор с представлением запросов (batch_size x len_q).\n",
    "    - seq_k: Тензор с представлением ключей (batch_size x len_k).\n",
    "    \n",
    "    Returns:\n",
    "    - pad_attn_mask: Маска для внимания, скрывающая PAD-токены (batch_size x 1 x len_k(=len_q)).\n",
    "    \"\"\"\n",
    "    batch_size, len_q = seq_q.size()\n",
    "    batch_size, len_k = seq_k.size()\n",
    "    \n",
    "    # Создаем маску, в которой значение True (1) будет соответствовать PAD-токенам.\n",
    "    pad_attn_mask = seq_k.data.eq(0).unsqueeze(1)  # batch_size x 1 x len_k(=len_q), где 1 означает маскирование\n",
    "    \n",
    "    # Расширяем маску до размерности batch_size x len_q x len_k, чтобы ее можно было использовать для маскирования внимания.\n",
    "    return pad_attn_mask.expand(batch_size, len_q, len_k)  # batch_size x len_q x len_k\n",
    "\n",
    "\n",
    "def gelu(x):\n",
    "    \"\"\"\n",
    "    Реализация активационной функции GELU (Gaussian Error Linear Unit) от Hugging Face.\n",
    "\n",
    "    Args:\n",
    "    - x: Входной тензор.\n",
    "\n",
    "    Returns:\n",
    "    - Результат применения функции GELU к входному тензору.\n",
    "    \"\"\"\n",
    "    return x * 0.5 * (1.0 + torch.erf(x / math.sqrt(2.0)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Embedding(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Embedding, self).__init__()\n",
    "        self.tok_embed = nn.Embedding(vocab_size, d_model)  # Токенное встраивание\n",
    "        self.pos_embed = nn.Embedding(maxlen, d_model)  # Встраивание позиции\n",
    "        self.seg_embed = nn.Embedding(n_segments, d_model)  # Встраивание сегмента (типа токена)\n",
    "        self.norm = nn.LayerNorm(d_model)\n",
    "\n",
    "    def forward(self, x, seg):\n",
    "        seq_len = x.size(1)\n",
    "        pos = torch.arange(seq_len, dtype=torch.long)\n",
    "        pos = pos.unsqueeze(0).expand_as(x)  # (seq_len,) -> (batch_size, seq_len)\n",
    "        # Общее встраивание, включая токены, позиции и сегменты\n",
    "        embedding = self.tok_embed(x) + self.pos_embed(pos) + self.seg_embed(seg)\n",
    "        return self.norm(embedding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "b237d4ea5134bb920276808a0b42c51d",
     "grade": true,
     "grade_id": "cell-cf85318c5b529648",
     "locked": false,
     "points": 0,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "## Задание 2 (1 балл): Реализация класса ScaledDotProductAttention\n",
    "\n",
    "**Цель задания:** Написать класс `ScaledDotProductAttention`, который реализует механизм внимания с масштабированным скалярным произведением.\n",
    "\n",
    "**Задачи:**\n",
    "1. Создать класс `ScaledDotProductAttention`, который наследует `nn.Module` из библиотеки PyTorch.\n",
    "2. В методе `forward` класса `ScaledDotProductAttention`, выполнить следующие действия:\n",
    "   - Вычислите оценки (scores) как скалярное произведение матрицы запросов (Q) и транспонированной матрицы ключей (K). \n",
    "   Масштабируйте оценки на обратный квадратный корень от размерности `d_k`.\n",
    "   - Примените маску внимания (`attn_mask`) для скрытия определенных значений в оценках (scores). \n",
    "   Для этого используйте метод `masked_fill_`, который заменяет элементы тензора на заданное значение, где маска равна 1.\n",
    "   - Примените функцию softmax для вычисления весов внимания, которые определяют, насколько каждый элемент входных данных важен.\n",
    "   - Вычислите контекст путем взвешенной суммы значений (V) с использованием весов внимания.\n",
    "   - Верните контекст и веса внимания.\n",
    "\n",
    "В случае затруднений можно обратиться к:\n",
    "https://github.com/graykode/nlp-tutorial/blob/master/5-2.BERT/BERT.py\n",
    "\"\"\"\n",
    "\n",
    "class ScaledDotProductAttention(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(ScaledDotProductAttention, self).__init__()\n",
    "\n",
    "    def forward(self, Q, K, V, attn_mask):\n",
    "        # НАЧАЛО ВАШЕГО РЕШЕНИЯ\n",
    "        raise NotImplementedError() # удалить эту строку в процессе решения\n",
    "        # КОНЕЦ ВАШЕГО РЕШЕНИЯ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "41f95d091fe36fc19f189fd8b9e7ff55",
     "grade": true,
     "grade_id": "cell-03e0e6e6528c749f",
     "locked": false,
     "points": 0,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "## Задание 3 (1 балл): Реализация класса MultiHeadAttention\n",
    "\n",
    "**Цель задания:** Написать класс `MultiHeadAttention`, который реализует многоголовое внимание (Multi-Head Attention) в модели трансформера.\n",
    "\n",
    "**Задачи:**\n",
    "1. Создать класс `MultiHeadAttention`, который наследует `nn.Module` из библиотеки PyTorch.\n",
    "2. В конструкторе класса `MultiHeadAttention`, определить следующие атрибуты:\n",
    "   - `W_Q`: Используйте линейный слой `nn.Linear` для создания преобразования запросов (Q). \n",
    "   Размерность выхода должна быть `d_k * n_heads`. Прокомментируйте это как \"Преобразование запросов\".\n",
    "   - `W_K`: Используйте линейный слой `nn.Linear` для создания преобразования ключей (K). \n",
    "   Размерность выхода должна быть `d_k * n_heads`. Прокомментируйте это как \"Преобразование ключей\".\n",
    "   - `W_V`: Используйте линейный слой `nn.Linear` для создания преобразования значений (V). \n",
    "   Размерность выхода должна быть `d_v * n_heads`. Прокомментируйте это как \"Преобразование значений\".\n",
    "3. В методе `forward` класса `MultiHeadAttention`, выполнить следующие действия:\n",
    "   - Сохраните оригинальные запросы `Q` и определите размер пакета (batch_size).\n",
    "   - Преобразуйте запросы, ключи и значения (`Q`, `K`, `V`) с использованием линейных преобразований `W_Q`, `W_K` и `W_V`, \n",
    "   учитывая многие головы. Результаты преобразований должны быть разделены на головы и транспонированы.\n",
    "   - Создайте маску внимания (`attn_mask`) с учетом входной маски `attn_mask` и повторите ее для каждой головы.\n",
    "   - Примените механизм внимания (`ScaledDotProductAttention`) к головам, передав преобразованные запросы, ключи, значения и маску внимания.\n",
    "   - Транспонируйте и объедините результаты голов для получения контекста.\n",
    "   - Примените линейное преобразование и слой нормализации к контексту.\n",
    "   - Верните нормализованный контекст и веса внимания.\n",
    "\n",
    "В случае затруднений можно обратиться к:\n",
    "https://github.com/graykode/nlp-tutorial/blob/master/5-2.BERT/BERT.py\n",
    "\"\"\"\n",
    "\n",
    "class MultiHeadAttention(nn.Module):\n",
    "    # НАЧАЛО ВАШЕГО РЕШЕНИЯ\n",
    "    raise NotImplementedError() # удалить эту строку в процессе решения\n",
    "    # КОНЕЦ ВАШЕГО РЕШЕНИЯ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "a8aa7542da018e4886bbb2201338e50c",
     "grade": true,
     "grade_id": "cell-0829ddc4353a996e",
     "locked": false,
     "points": 0,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "## Задание 4 (1 балл): Реализация класса PoswiseFeedForwardNet\n",
    "\n",
    "**Цель задания:** Написать класс `PoswiseFeedForwardNet`, который реализует полносвязную нейронную сеть с прямым распространением \n",
    "(Feed Forward Neural Network) в модели трансформера.\n",
    "\n",
    "**Задачи:**\n",
    "1. Создать класс `PoswiseFeedForwardNet`, который наследует `nn.Module` из библиотеки PyTorch.\n",
    "2. В конструкторе класса `PoswiseFeedForwardNet`, определить два атрибута:\n",
    "   - `fc1`: Используйте линейный слой `nn.Linear`, чтобы создать первый линейный слой прямого распространения (feed-forward). \n",
    "   Размерность входа должна быть `d_model`, а размерность выхода - `d_ff`. Прокомментируйте это как \"Первый линейный слой (прямого распространения)\".\n",
    "   - `fc2`: Используйте линейный слой `nn.Linear`, чтобы создать второй линейный слой прямого распространения. \n",
    "   Размерность входа должна быть `d_ff`, а размерность выхода - `d_model`. Прокомментируйте это как \"Второй линейный слой (прямого распространения)\".\n",
    "3. В методе `forward` класса `PoswiseFeedForwardNet`, выполнить следующие действия:\n",
    "   - Пропустите входные данные `x` через первый линейный слой `fc1`.\n",
    "   - Примените функцию активации GELU (Gaussian Error Linear Unit) к результату первого линейного слоя.\n",
    "   - Пропустите результат через второй линейный слой `fc2`.\n",
    "   - Верните полученный результат в качестве выходных данных модели.\n",
    "\n",
    "В случае затруднений можно обратиться к:\n",
    "https://github.com/graykode/nlp-tutorial/blob/master/5-2.BERT/BERT.py\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "class PoswiseFeedForwardNet(nn.Module):\n",
    "    # НАЧАЛО ВАШЕГО РЕШЕНИЯ\n",
    "    raise NotImplementedError() # удалить эту строку в процессе решения\n",
    "    # КОНЕЦ ВАШЕГО РЕШЕНИЯ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "31b75810554dc7ff82fdf946ed9ee2d8",
     "grade": true,
     "grade_id": "cell-4f4e36c132987802",
     "locked": false,
     "points": 0,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "## Задание 5 (1 балл): Реализация класса EncoderLayer\n",
    "\n",
    "**Цель задания:** Написать класс `EncoderLayer`, который представляет собой один слой кодировщика в модели трансформера.\n",
    "\n",
    "**Задачи:**\n",
    "1. Создать класс `EncoderLayer`, который наследует `nn.Module` из библиотеки PyTorch.\n",
    "2. В конструкторе класса `EncoderLayer`, определить два атрибута:\n",
    "   - `enc_self_attn`: Используйте класс `MultiHeadAttention`, чтобы создать многоголовое внимание для кодировщика. \n",
    "   Прокомментируйте это как \"Многоголовое внимание для кодировщика\".\n",
    "   - `pos_ffn`: Используйте класс `PoswiseFeedForwardNet`, чтобы создать полносвязную нейронную сеть с прямым распространением. \n",
    "   Прокомментируйте это как \"Полносвязная нейронная сеть с прямым распространением\".\n",
    "3. В методе `forward` класса `EncoderLayer`, выполнить следующие действия:\n",
    "   - Применить многоголовое внимание `enc_self_attn` для самокодирования кодировщика, используя входные данные \n",
    "   `enc_inputs` в роли запросов, ключей и значений.\n",
    "   - Применить полносвязную нейронную сеть с прямым распространением `pos_ffn` для обработки выходных данных многоголового внимания.\n",
    "   - Вернуть обработанные данные и веса внимания.\n",
    "\n",
    "В случае затруднений можно обратиться к:\n",
    "https://github.com/graykode/nlp-tutorial/blob/master/5-2.BERT/BERT.py\n",
    "\"\"\"\n",
    "\n",
    "class EncoderLayer(nn.Module):\n",
    "    # НАЧАЛО ВАШЕГО РЕШЕНИЯ\n",
    "    raise NotImplementedError() # удалить эту строку в процессе решения\n",
    "    # КОНЕЦ ВАШЕГО РЕШЕНИЯ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "d98a6f55fb52dc4c307cba8439011204",
     "grade": true,
     "grade_id": "cell-81ccedae031e4069",
     "locked": false,
     "points": 0,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "**Задание 6 (1 балл): Реализация модели BERT**\n",
    "\n",
    "**Цель:** Ваша задача - реализовать модель BERT (Bidirectional Encoder Representations from Transformers) с использованием PyTorch.\n",
    "\n",
    "**Инструкции:**\n",
    "\n",
    "1. Ваша задача - реализовать класс BERT.\n",
    "\n",
    "Описание задания для студентов: Реализация модели BERT\n",
    "\n",
    "1. Создайте класс `BERT`, который будет наследоваться от `nn.Module`. В этом классе вы должны реализовать архитектуру BERT.\n",
    "\n",
    "2. В конструкторе класса `BERT`, выполните следующие шаги:\n",
    "   - Создайте модуль встраивания (Embedding) и добавьте его в класс.\n",
    "   - Создайте список слоев кодировщика (EncoderLayer) и добавьте его в класс. Количество слоев кодировщика (`n_layers`) можно задать в качестве аргумента конструктора.\n",
    "   - Создайте линейный слой (`nn.Linear`) для извлечения признаков из первого токена (CLS) и добавьте функцию активации (гиперболический тангенс).\n",
    "   - Создайте второй линейный слой для извлечения скрытых признаков из выхода трансформера на позициях маскированных токенов. В качестве функции активации используйте GELU (Gaussian Error Linear Unit).\n",
    "   - Добавьте слой нормализации (LayerNorm) и линейный классификатор для задачи классификации.\n",
    "\n",
    "3. Реализуйте метод `forward`, который принимает входные данные (`input_ids`, `segment_ids`, `masked_pos`) и выполняет следующие действия:\n",
    "   - Применяет входные данные через встраивание и кодировщик.\n",
    "   - Извлекает признаки из первого токена (CLS) для классификации.\n",
    "   - Извлекает скрытые признаки из выхода трансформера на позициях маскированных токенов.\n",
    "   - Возвращает результаты классификации и предсказания маскированных токенов.\n",
    "\n",
    "4. Обратите внимание на использование функций активации, слоев нормализации и линейных слоев в методе `forward`.\n",
    "\n",
    "5. Создайте декодер, который будет использоваться совместно с встраиванием для задачи предсказания маскированных токенов.\n",
    "\n",
    "6. Установите веса декодера равными весам встраивания.\n",
    "\n",
    "7. Возвращайте результаты предсказания маскированных токенов и классификации.\n",
    "\n",
    "В случае затруднений можно обратиться к:\n",
    "https://github.com/graykode/nlp-tutorial/blob/master/5-2.BERT/BERT.py\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "class BERT(nn.Module):\n",
    "    # НАЧАЛО ВАШЕГО РЕШЕНИЯ\n",
    "    raise NotImplementedError() # удалить эту строку в процессе решения\n",
    "    # КОНЕЦ ВАШЕГО РЕШЕНИЯ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Параметры BERT\n",
    "maxlen = 30  # максимальная длина\n",
    "batch_size = 6\n",
    "max_pred = 5  # максимальное количество предсказываемых токенов\n",
    "n_layers = 6  # количество слоев в кодировщике\n",
    "n_heads = 12  # количество голов в Multi-Head Attention\n",
    "d_model = 768  # размер вложения\n",
    "d_ff = 768 * 4  # 4*d_model, размер скрытого слоя FeedForward\n",
    "d_k = d_v = 64  # размерности K(=Q), V\n",
    "n_segments = 2\n",
    "\n",
    "text = (\n",
    "    'Привет, как дела? Я - Ромео.\\n'\n",
    "    'Привет, Ромео. Меня зовут Джульетта. Приятно познакомиться.\\n'\n",
    "    'Приятно познакомиться. Как твои дела сегодня?\\n'\n",
    "    'Здорово. Моя бейсбольная команда выиграла соревнование.\\n'\n",
    "    'Поздравляю, Джульетта.\\n'\n",
    "    'Спасибо, Ромео'\n",
    ")\n",
    "sentences = re.sub(\"[.,!?\\\\-]\", '', text.lower()).split('\\n')  # фильтруем '.', ',', '?', '!'\n",
    "word_list = list(set(\" \".join(sentences).split()))\n",
    "word_dict = {'[PAD]': 0, '[CLS]': 1, '[SEP]': 2, '[MASK]': 3}\n",
    "for i, w in enumerate(word_list):\n",
    "    word_dict[w] = i + 4\n",
    "number_dict = {i: w for i, w in enumerate(word_dict)}\n",
    "vocab_size = len(word_dict)\n",
    "\n",
    "token_list = list()\n",
    "for sentence in sentences:\n",
    "    arr = [word_dict[s] for s in sentence.split()]\n",
    "    token_list.append(arr)\n",
    "\n",
    "model = BERT()\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "batch = make_batch()\n",
    "input_ids, segment_ids, masked_tokens, masked_pos, isNext = map(torch.LongTensor, zip(*batch))\n",
    "\n",
    "for epoch in range(100):\n",
    "    optimizer.zero_grad()\n",
    "    logits_lm, logits_clsf = model(input_ids, segment_ids, masked_pos)\n",
    "    loss_lm = criterion(logits_lm.transpose(1, 2), masked_tokens)  # для маскированной модели языка\n",
    "    loss_lm = (loss_lm.float()).mean()\n",
    "    loss_clsf = criterion(logits_clsf, isNext)  # для классификации предложений\n",
    "    loss = loss_lm + loss_clsf\n",
    "    if (epoch + 1) % 10 == 0:\n",
    "        print('Эпоха:', '%04d' % (epoch + 1), 'функция потерь =', '{:.6f}'.format(loss))\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "# Предсказание маскированных токенов и isNext\n",
    "input_ids, segment_ids, masked_tokens, masked_pos, isNext = map(torch.LongTensor, zip(batch[0]))\n",
    "print(text)\n",
    "print([number_dict[w.item()] for w in input_ids[0] if number_dict[w.item()] != '[PAD]'])\n",
    "\n",
    "logits_lm, logits_clsf = model(input_ids, segment_ids, masked_pos)\n",
    "logits_lm = logits_lm.data.max(2)[1][0].data.numpy()\n",
    "print('Список маскированных токенов: ', [pos.item() for pos in masked_tokens[0] if pos.item() != 0])\n",
    "print('Предсказанный список маскированных токенов: ', [pos for pos in logits_lm if pos != 0])\n",
    "\n",
    "logits_clsf = logits_clsf.data.max(1)[1].data.numpy()[0]\n",
    "print('isNext : ', True if isNext else False)\n",
    "print('Предсказано isNext : ', True if logits_clsf else False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Импорт необходимых библиотек\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from transformers import AdamW\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report\n",
    "import pandas as pd\n",
    "\n",
    "# Загрузка датасета \"20 Newsgroups\"\n",
    "from sklearn.datasets import fetch_20newsgroups\n",
    "newsgroups_data = fetch_20newsgroups(subset='all', remove=('headers', 'footers', 'quotes'))\n",
    "df = pd.DataFrame({'text': newsgroups_data.data, 'label': newsgroups_data.target})\n",
    "\n",
    "# Разделение данных на обучающий и тестовый наборы\n",
    "train_data, test_data, train_labels, test_labels = train_test_split(df['text'], df['label'], test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "080e6b778d26d400cdfaa5174844ac96",
     "grade": true,
     "grade_id": "cell-ffd6dd212fc6d339",
     "locked": false,
     "points": 0,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "**Задание 7 (3 балла): Реализация модели BERT из библиотеки transformers**\n",
    "\n",
    "**Цель:** Ваша задача - реализовать модель BERT с использованием библиотеки transformers на наборе данных 20newsgroups.\n",
    "Вывести на печать метрики модели на тестовом наборе test_data с помощью classification_report\n",
    "\n",
    "В случае затруднений можно обратиться к:\n",
    "https://huggingface.co/docs/transformers/model_doc/bert\n",
    "\"\"\"\n",
    "\n",
    "# Загрузка предварительно обученного BERT\n",
    "# НАЧАЛО ВАШЕГО РЕШЕНИЯ\n",
    "raise NotImplementedError() # удалить эту строку в процессе решения\n",
    "# КОНЕЦ ВАШЕГО РЕШЕНИЯ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "7794d2291628850022278c8953d59de9",
     "grade": true,
     "grade_id": "cell-bb2981f26e950850",
     "locked": false,
     "points": 0,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "**Задание 8 (3 балла): Реализация модели BART из библиотеки transformers**\n",
    "\n",
    "**Цель:** Ваша задача - реализовать модель BART с использованием библиотеки transformers на наборе данных 20newsgroups.\n",
    "Вывести на печать метрики модели на тестовом наборе test_data с помощью classification_report\n",
    "\n",
    "В случае затруднений можно обратиться к:\n",
    "https://huggingface.co/docs/transformers/model_doc/bart\n",
    "\"\"\"\n",
    "\n",
    "# Загрузка предварительно обученного BART\n",
    "# НАЧАЛО ВАШЕГО РЕШЕНИЯ\n",
    "raise NotImplementedError() # удалить эту строку в процессе решения\n",
    "# КОНЕЦ ВАШЕГО РЕШЕНИЯ"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
